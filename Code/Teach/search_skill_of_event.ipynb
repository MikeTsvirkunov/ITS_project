{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from json import load\n",
    "from spacy.util import minibatch, compounding\n",
    "import spacy\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, RocCurveDisplay, auc, mean_squared_error\n",
    "from sklearn.preprocessing import normalize\n",
    "import re\n",
    "from spacy.training.example import Example\n",
    "nlp = spacy.load(\"ru_core_news_md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../Data/Events/type_of_events.json', 'r', encoding='utf-8') as js:\n",
    "    discription_of_event = load(js)\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "punctuation = list(map(lambda a: a, punctuation)) + \\\n",
    "    ['(', ')', '\"', '«', '»', \"'\", \"✅\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_describer(text, ent):\n",
    "    return (text, {\"entities\": ent})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "\n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA = []\n",
    "for events in discription_of_event:\n",
    "    description = discription_of_event[events][\"описание\"]\n",
    "    new_skills = discription_of_event[events][\"new_skills\"]\n",
    "    for sentence in description.replace('\\n', ' ').replace('✅', ' ').split(\".\"):\n",
    "        for skill in new_skills:\n",
    "            if skill in sentence:\n",
    "                TRAIN_DATA.append(to_describer(sentence, [(position.start(), position.end()-1, \"type\") for position in re.finditer(skill, sentence)]))\n",
    "        \n",
    "len(TRAIN_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = trim_entity_spans(TRAIN_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['    Как найти исполнительного директора и передать оперативное управление', {'entities': [[8, 39, 'type']]}]\n",
      "найти исполнительного директора\n",
      "['    Как найти исполнительного директора и передать оперативное управление', {'entities': [[42, 72, 'type']]}]\n",
      "передать оперативное управлени\n"
     ]
    }
   ],
   "source": [
    "for i in TRAIN_DATA:\n",
    "    if 'Как найти исполнительного директора и передать' in i[0]:\n",
    "        print(i)\n",
    "        print(i[0][i[1]['entities'][0][0]:i[1]['entities'][0][1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', '-', '-', '-', '-', '-', 'O', 'O']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.training.offsets_to_biluo_tags(nlp.make_doc(\n",
    "    'Как найти исполнительного директора и передать оперативное управление'), [(8, 39, 'type')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Как найти исполнительного директора и передать оперативное управление"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.make_doc(\n",
    "    'Как найти исполнительного директора и передать оперативное управление')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('TOKEN', 'Как'), ('TOKEN', 'найти'), ('TOKEN', 'исполнительного'), ('TOKEN', 'директора'), ('TOKEN', 'и'), ('TOKEN', 'передать'), ('TOKEN', 'оперативное'), ('TOKEN', 'управление')]\n",
      "[('TOKEN', 'Как'), ('TOKEN', 'найти'), ('TOKEN', 'исполнительного'), ('TOKEN', 'директора'), ('TOKEN', 'и'), ('TOKEN', 'передать'), ('TOKEN', 'оперативное'), ('TOKEN', 'управление')]\n"
     ]
    }
   ],
   "source": [
    "for i in TRAIN_DATA:\n",
    "    if 'исполнительного директора и передать оперативное' in i[0]:\n",
    "        print(nlp.tokenizer.explain(i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E024] Could not find an optimal move to supervise the parser. Usually, this means that the model can't be updated in a way that's valid and satisfies the correct annotations specified in the GoldParse. For example, are all labels added to the model? If you're training a named entity recognizer, also make sure that none of your annotated entity spans have leading or trailing whitespace or punctuation. You can also use the `debug data` command to validate your JSON-formatted training data. For details, run:\npython -m spacy debug data --help",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m example \u001b[39m=\u001b[39m Example\u001b[39m.\u001b[39mfrom_dict(doc, annotations)\n\u001b[0;32m     14\u001b[0m \u001b[39m# print(losses)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m nlp\u001b[39m.\u001b[39;49mupdate([example], losses\u001b[39m=\u001b[39;49mlosses, drop\u001b[39m=\u001b[39;49m\u001b[39m0.0001\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Mike\\Desktop\\ITS_project\\ITS_project\\lib\\site-packages\\spacy\\language.py:1155\u001b[0m, in \u001b[0;36mLanguage.update\u001b[1;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[0;32m   1152\u001b[0m \u001b[39mfor\u001b[39;00m name, proc \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline:\n\u001b[0;32m   1153\u001b[0m     \u001b[39m# ignore statements are used here because mypy ignores hasattr\u001b[39;00m\n\u001b[0;32m   1154\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(proc, \u001b[39m\"\u001b[39m\u001b[39mupdate\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1155\u001b[0m         proc\u001b[39m.\u001b[39mupdate(examples, sgd\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, losses\u001b[39m=\u001b[39mlosses, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcomponent_cfg[name])  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m     \u001b[39mif\u001b[39;00m sgd \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   1157\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1158\u001b[0m             name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude\n\u001b[0;32m   1159\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(proc, ty\u001b[39m.\u001b[39mTrainableComponent)\n\u001b[0;32m   1160\u001b[0m             \u001b[39mand\u001b[39;00m proc\u001b[39m.\u001b[39mis_trainable\n\u001b[0;32m   1161\u001b[0m             \u001b[39mand\u001b[39;00m proc\u001b[39m.\u001b[39mmodel \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, \u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   1162\u001b[0m         ):\n",
      "File \u001b[1;32mc:\\Users\\Mike\\Desktop\\ITS_project\\ITS_project\\lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:397\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.update\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Mike\\Desktop\\ITS_project\\ITS_project\\lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:653\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser._init_gold_batch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Mike\\Desktop\\ITS_project\\ITS_project\\lib\\site-packages\\spacy\\pipeline\\_parser_internals\\transition_system.pyx:132\u001b[0m, in \u001b[0;36mspacy.pipeline._parser_internals.transition_system.TransitionSystem.get_oracle_sequence_from_state\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E024] Could not find an optimal move to supervise the parser. Usually, this means that the model can't be updated in a way that's valid and satisfies the correct annotations specified in the GoldParse. For example, are all labels added to the model? If you're training a named entity recognizer, also make sure that none of your annotated entity spans have leading or trailing whitespace or punctuation. You can also use the `debug data` command to validate your JSON-formatted training data. For details, run:\npython -m spacy debug data --help"
     ]
    }
   ],
   "source": [
    "for _, annotations in TRAIN_DATA:\n",
    "  for ent in annotations.get(\"entities\"):\n",
    "    ner.add_label(ent[2])\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# shuffle(TRAIN_DATA)\n",
    "\n",
    "losses = {}\n",
    "for batch in spacy.util.minibatch(TRAIN_DATA, size=compounding(30.0, 50, 5.5)):\n",
    "    for text, annotations in batch:\n",
    "        doc = nlp(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        # print(losses)\n",
    "        nlp.update([example], losses=losses, drop=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(антон петроченко,)\n",
      "(александр гунгер,)\n",
      "(алексей бондаренко,)\n",
      "(российской федерации, ано)\n",
      "(тпп, омской области)\n",
      "(тпп, омской области)\n",
      "(тпп, омской области)\n",
      "(тпп, омской области)\n",
      "(тпп, омской области)\n",
      "(тпп, омской области)\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "(рф, ржд, роскосмоса)\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "(кристина казазаева, aiesec)\n",
      "(москвы,)\n",
      "(москвы,)\n",
      "(алина судоплатова, проведёт)\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "(москвы, ооо \"спотзания\"- центра дополнительного образования)\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# doc1 = nlp.make_doc(\n",
    "#     'Как найти исполнительного директора и передать оперативное управление'.lower())\n",
    "# doc1.ents\n",
    "for i in TRAIN_DATA:\n",
    "    print(nlp(i[0].lower()).ents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITS_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
